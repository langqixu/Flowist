
import asyncio
import sys
import os

# Add project root to path
sys.path.append(os.getcwd())

from app.rag_service.retriever import KnowledgeRetriever
from app.core.llm_client import LLMClient
from app.config import get_settings

async def test_full_flow():
    print("ğŸš€ å¼€å§‹ Flowist ç³»ç»Ÿè‡ªæ£€...")
    print("-" * 50)

    # 1. éªŒè¯é…ç½®
    print("1ï¸âƒ£  æ£€æŸ¥é…ç½®...")
    settings = get_settings()
    print(f"   LLM Model: {settings.openai_model}")
    print(f"   Base URL: {settings.openai_base_url}")
    if not settings.openai_api_key:
        print("   âŒ Error: OPENAI_API_KEY æœªè®¾ç½®")
        return
    print("   âœ… é…ç½®è¯»å–æˆåŠŸ")

    # 2. éªŒè¯ RAG
    print("\n2ï¸âƒ£  éªŒè¯ RAG çŸ¥è¯†æ£€ç´¢...")
    try:
        retriever = KnowledgeRetriever()
        query = "æˆ‘æœ€è¿‘å‹åŠ›å¾ˆå¤§ï¼Œå¤±çœ "
        results = retriever.retrieve_knowledge(query, n_results=2)
        if "No relevant knowledge found" in results or not results:
            print("   âš ï¸ Warning: æœªæ£€ç´¢åˆ°çŸ¥è¯† (å¯èƒ½æ˜¯çŸ¥è¯†åº“ä¸ºç©º)")
        else:
            print(f"   âœ… æ£€ç´¢æˆåŠŸã€‚Query: '{query}'")
            print("   --- æ£€ç´¢ç‰‡æ®µé¢„è§ˆ ---")
            print(results[:200] + "...")
            print("   ------------------")
    except Exception as e:
        print(f"   âŒ RAG Error: {str(e)}")
        return

    # 3. éªŒè¯ LLM è¿æ¥
    print("\n3ï¸âƒ£  éªŒè¯ LLM ç”Ÿæˆ (DeepSeek)...")
    try:
        client = LLMClient()
        prompt = "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»å†¥æƒ³çš„å¥½å¤„ã€‚"
        print(f"   å‘é€ Prompt: '{prompt}'")
        response = await client.generate(prompt)
        print(f"   âœ… LLM å“åº”æˆåŠŸ: {response}")
    except Exception as e:
        print(f"   âŒ LLM Error: {str(e)}")
        return

    print("\n" + "=" * 50)
    print("ğŸ‰ ç³»ç»Ÿè‡ªæ£€é€šè¿‡ï¼ä¸€åˆ‡å‡†å¤‡å°±ç»ªã€‚")
    print("=" * 50)

if __name__ == "__main__":
    asyncio.run(test_full_flow())
